 # Import necessary libraries
 import tensorflow as tf
 from tensorflow.keras.layers import Dense, Flatten, Reshape, LeakyReLU, BatchNormalization, Conv2D, Conv2DTranspose
 from tensorflow.keras.models import Sequential
 from tensorflow.keras.optimizers import Adam
 import numpy as np
 import matplotlib.pyplot as plt
 import os
 # Load and preprocess the dataset (MNIST dataset)
 (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
 x_train = x_train / 255.0
 x_train = np.expand_dims(x_train, axis=-1)
 # Define the Generator model
 def build_generator():
    model = Sequential([
        Dense(256, input_dim=100),
        LeakyReLU(alpha=0.2),
        BatchNormalization(momentum=0.8),
        Dense(512),
        LeakyReLU(alpha=0.2),
        BatchNormalization(momentum=0.8),
        Dense(1024),
        LeakyReLU(alpha=0.2),
        BatchNormalization(momentum=0.8),
        Dense(28*28*1, activation='tanh'),
        Reshape((28, 28, 1))
    ])
    return model
 # Define the Discriminator model
 def build_discriminator():
    model = Sequential([
        Flatten(input_shape=(28, 28, 1)),
        Dense(512),
        LeakyReLU(alpha=0.2),
        Dense(256),
        LeakyReLU(alpha=0.2),
        Dense(1, activation='sigmoid')
    ])
    return model
 # Compile the Discriminator
 discriminator = build_discriminator()
 discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
 # Compile the combined GAN model
 generator = build_generator()
 z = tf.keras.Input(shape=(100,))
 img = generator(z)
 discriminator.trainable = False
 valid = discriminator(img)
 combined = tf.keras.Model(z, valid)
 combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
 # Train the GAN
 def train_gan(epochs, batch_size=128, save_interval=200):
    # Load and preprocess the dataset
    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
    x_train = (x_train.astype(np.float32) - 127.5) / 127.5
    x_train = np.expand_dims(x_train, axis=-1)
    # Adversarial ground truths
    valid = np.ones((batch_size, 1))
    fake = np.zeros((batch_size, 1))
    for epoch in range(epochs):
        # --------------------
        # Train Discriminator
        # --------------------
        # Select a random half batch of images
        idx = np.random.randint(0, x_train.shape[0], batch_size)
        imgs = x_train[idx]
        # Sample noise and generate a batch of new images
        noise = np.random.normal(0, 1, (batch_size, 100))
        gen_imgs = generator.predict(noise)
        # Train the discriminator (real classified as 1 and fake as 0)
        d_loss_real = discriminator.train_on_batch(imgs, valid)
        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
        # --------------------
        # Train Generator
        # --------------------
        noise = np.random.normal(0, 1, (batch_size, 100))
        # Train the generator (wants discriminator to mistake images as real)
        g_loss = combined.train_on_batch(noise, valid)
        # Print the progress
        print(f"{epoch} [D loss: {d_loss[0]}] [G loss: {g_loss}]")
        # If at save interval, save generated image samples
        if epoch % save_interval == 0:
            save_imgs(epoch)
 # Generate and save images
 def save_imgs(epoch):
    # Create the directory if it doesn't exist
    os.makedirs("images", exist_ok=True)
    r, c = 5, 5
    noise = np.random.normal(0, 1, (r * c, 100))
    gen_imgs = generator.predict(noise)
    # Rescale images 0 - 1
    gen_imgs = 0.5 * gen_imgs + 0.5
    fig, axs = plt.subplots(r, c)
    cnt = 0
    for i in range(r):
        for j in range(c):
            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')
            axs[i, j].axis('off')
            cnt += 1
    fig.savefig(f"images/mnist_{epoch}.png")
    plt.close()
 # Train the GAN model
 train_gan(epochs=10, batch_size=6, save_interval=1)